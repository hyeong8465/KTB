{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNtIefoKxeeR0VZF46AhVpL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hyeong8465/KTB/blob/main/ktb_ai/ktb_day0724.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M41esI1REIsu"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "딥러닝 4\n",
        "\n",
        "- PyTorch 기초지식\n",
        "동적 계산 그래프, 간편한 디버깅, Pythonic한 코드\n",
        "\n",
        "텐서: 다차원 배열 또는 행렬과 유사, ndarray와 유사\n",
        "\n",
        "- Seq2Seq 모델\n",
        "입력 시퀀스를 출력 시퀀스로 변환하는 데 사용되는 딥러닝 모델\n",
        "기계 번역, 텍스트 요약, 대화 시스템 등에서 사용\n",
        "인코더와 디코더로 구성\n",
        "\n",
        "인코더\n",
        "입력 시퀀스를 고정된 길이의 벡터로 변환(RNN, LSTM, GRU와 같은 순환 신경망으로 구성)\n",
        "작동 원리: 입력 시퀀스를 한 요소씩 입력, 각 요소를 처리하면서 은닉 상태 업데이트, 마지막 요소가 처리되면 마지막 은식 상태를 고정된 길이의 컨텍스트 벡터로 출력\n",
        "\n",
        "디코더\n",
        "인코더에서 생성된 고정된 길이의 벡터를 입력으로 받아 출력 시퀀스를 생성(RNN, LSTM, GRU와 같은 순환 신경망으로 구성)\n",
        "작동원리: 인코더의 마지막 은닉 상태를 초기 히든 상태로 사용, 시작 토큰을 입력으로 받아 다음 토큰을 예측, 예측된 토큰은 다시 디코더의 입력으로 사용되어 다음 토큰을 예측하는 과정을 반복, 훈련과정에서는 정답값을 입력으로 사용\n",
        "\n",
        "Teacher Forcing\n",
        "실제 목표 시퀀스의 요소를 다음 입력으로 사용하는 방법\n",
        "모델이 더 빠르고 안정적으로 수렴할 수 있도록 보조\n",
        "\n",
        "Seq2Seq장점: 입출력 시퀀스의 길이에 유연, 자연어 처리에 특화, 넓은 응용 분야\n",
        "Seq2Seq단점: 긴 시퀀스를 처리할 때 중요한 정보 손실 가능, 많은 데이터와 계산, 병렬화 어려움, 초기 단계에서의 오류가 전체 시퀀스 품질에 영향\n",
        "\n",
        "- 오토 인코더\n",
        "데이터의 효율적인 표현을 학습하는 비지도 학습 인공신경망 모델\n",
        "데이터 압축, 차원 축소, 노이즈 제거, 이상 탐지 등에 사용\n",
        "입력 데이터를 압축하여 잠재 공간 표현으로 변환 후, 다시 원래 데이터로 복원하는 과정을 통해 학습\n",
        "\n",
        "오토인코더의 구조\n",
        "인코더: 입력 데이터를 저차원 잠재 공간 백터로 변환, 입력층에서 잠재 공간 표현까지의 여러 층의 신경망으로 구성\n",
        "디코더: 잠재 공간 벡터를 원래 데이터로 복원, 잠재 공간 표현에서 출력층까지의 여러 층의 신경망으로 구성\n",
        "\n",
        "오토인코더의 유형: 기본, 딥, 희소, 변이, 잡음 제거\n",
        "응용: 데이터 압축, 차원 축소, 노이즈 제거, 이상 탐지, 생성 모델\n",
        "\n",
        "장점: 레이블이 없는 데이터로 학습 가능, 다양한 응용 분야, 잠재 공간 벡터를 분석하여 데이터 특징 파악\n",
        "단점: 복원된 데이터와 원본 데이터가 정확히 일치하지 않을 수 있다. 매우 복잡한 데이터에 대해서 처리가 어렵다. 적절한 정규화가 없으면 과적합 발생할 수 있다.\n",
        "\n",
        "- 생성적 적대 신경망(Generative Adversarial Networks, GAN)\n",
        "두 개의 신경망, 생성자와 판별자가 경쟁적으로 학습하여 현실과 유사한 데이터를 생성하는 모델\n",
        "이미지 생성, 데이터 증강, 비디오 생성 등 다양한 응용 분야에서 사용\n",
        "\n",
        "구성요소\n",
        "생성자: 임의의 노이즈 벡터를 받아들여 현실감 있는 데이터를 생성, 입력 노이즈 벡터를 고차원 데이터로 변환, 생성된 데이터를 실제 데이터처럼 보이게 하는 것이 목표\n",
        "판별자: 실제 데이터와 생성된 데이터를 구분하는 역할, 입력된 데이터를 분류하는 신경망\n",
        "\n",
        "학습절차\n",
        "생성자는 노이즈 벡터를 통해 가짜 데이터 생성\n",
        "판별자는 실제 데이터와 생성된\n",
        "\n",
        "변형모델: DCGAN, CycleGAN, StyleGAN\n",
        "응용분야: 이미지 및 비디오 생성, 데이터 증강, 텍스트 생성, 이미지 변환\n",
        "장점: 현실과 유사한 데이터 생성, 특정 문제에 더 잘 맞는 모델을 개발 및 선택 가능, 무한히 새로운 데이터 생성 가능\n",
        "단점: 학습 과정이 불안정(생성자와 판별자의 불균형), 제한된 종류의 데이터만 생성하게 되는 문제 발생 가능, 많은 계산 자원과 시간 필요\n",
        "\n",
        "- 딥러닝 모델 최적화\n",
        "1. 학습률\n",
        "가중치가 업데이트되는 크기를 조정하는 하이퍼파라미터\n",
        "설정 방법: 실험적으로 설정하고 성능을 기준으로 조정, 0.001 혹은 0.01부터 시작, 학습률 스케줄링\n",
        "\n",
        "2. 배치 사이즈\n",
        "한 번의 업데이트에 사용되는 훈련 샘플의 수\n",
        "설정방법: 32, 64, 128, 256과 같은 2의 제곱수로 설정, 메모리와 속도를 고려하여 적절하게 선택\n",
        "\n",
        "3. 드롭아웃 비율(dropout rate)\n",
        "각 학습 단계에서 무작위로 비활성화할 뉴런의 비율\n",
        "설정방법: 일반적으로 0.2~0.5 사이의 값을 사용, 실험적으로 선정하고 성능을 기준으로 조정\n",
        "\n",
        "4. 정규화 방법\n",
        "손실 함수에 정규화 항을 추가하여 모델의 복잡성을 제어하고 과적합을 방지\n",
        "L1 정규화: 가중치의 절대값 합을 손실 함수에 추가, 가중치를 희소하게 만들어 일부 가중치를 0으로 만듦\n",
        "L2 정규화: 가중치의 제곱 합을 손실 함수에 추가, 가중치의 크기를 줄여 과적합 방지\n",
        "설정방법: lambda값을 실험적으로 설정하고 모델의 성능을 기준으로 조정, L2 정규화 더 자주 사용\n",
        "\n",
        "5. 네트워크 깊이와 폭\n",
        "깊이는 신경망의 층, 폭은 뉴런 수\n",
        "깊은 네트워크: 더 복잡한 패턴 학습, 기울기 소실/폭발 문제와 과적합의 위험\n",
        "넓은 네트워크: 더 많은 특징을 학습할 수 있지만, 많은 계산\n",
        "설정방법: 문제의 복잡성과 데이터셋의 크기에 따라 적절한 깊이와 폭 선택, 드롭아웃, 정규화와 함께 사용\n",
        "\n",
        "6. epoch\n",
        "전체 데이터셋을 몇 번 반복해서 학습했는지를 의미\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 실습\n",
        "1. pytorch의 기본 개념이해, 텐서 연산, 자동 미분, 간단한 신경망 모델 구현\n",
        "2. 간단한 seq2seq 모델 구현\n",
        "3. 실제 데이터 셋 사용\n",
        "4. 생성한 모델들에 다양한 학습률, 배치크기, 드롭아웃 비율들을 조절해서 성능 변화 비교\n"
      ],
      "metadata": {
        "id": "omxhKiFPaQr6"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eXtxTXJvaPY7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}