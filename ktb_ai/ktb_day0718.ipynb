{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOsrY2cc6aVBycQpmEz21A2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hyeong8465/KTB/blob/main/ktb_ai/ktb_day0718.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TkJ027ACKO_D"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "딥러닝 1\n",
        "- 딥러닝\n",
        "안간의 두뇌를 모방한 구조로, 다층 신경망을 사용하여 데이터로부터 특징을 학습하고 패턴을 인식하는 기술\n",
        "복잡한 데이터 분석과 패턴 인식에서 뛰어난 성능\n",
        "\n",
        "피처 엔지니어링을 최소화하고, 엔드 투 엔드 학습을 진행\n",
        "모델의 복잡성, 대량의 데이터, 자동으로 특징 추출, 긴 학습 시간, 대량의 데이터와 복잡한 문제에서 뛰어난 성능\n",
        "\n",
        "AlexNet의 ImageNet 대회 우승, AlphaGo의 바둑 대결\n",
        "\n",
        "- 딥러닝 기초\n",
        "뉴런: 입력 신호를 받아 가중치와 함께 처리한 후, 활성화 함수를 통해 출력 신호 생성, 주어진 입력 데이터에서 특정 패턴이나 특징 학습\n",
        "입력, 가중치, 바이어스, 활성화 함수\n",
        "작동 원리: 입력 신호 수집, 가중치와 입력 값 곱셈, 가중합 계산, 바이어스 추가, 활성화 함수 적용\n",
        "\n",
        "활성화 함수: 신경망이 복합한 패턴을 학습하고, 비선형 문제를 해결할 수 있게 한다.\n",
        "sigmoid, relu, tanh\n",
        "\n",
        "신경망의 구조: 입력층, 은닉층, 출력층\n",
        "\n",
        "- 딥러닝 원리\n",
        "딥러닝 학습과정: feepforward, lossfunction, backpropagation\n",
        "학습방법: 배치 학습, 미니 배치 학습\n",
        "최적화 알고리즘: 경사 하강법, 확률적 경사 하강법, Adam\n",
        "\n",
        "손실함수: 모델의 예측 값과 실제 값 간의 차이를 측정하는 함수\n",
        "손실함수 역할: 손실함수를 최소화하는 방향으로 학습 과정에서 가중치를 업데이트\n",
        "교차 엔트로피: 분류 문제에서 주로 사용\n",
        "장점: 확률 분포를 다루기 때문에 출력을 확률로 해석 가능\n",
        "단점: 잘못된 예측을 할 때 큰 손실 값을 가지기 때문에, 초기 학습 단계에서 손실 값이 크게 나타날 수 있음\n",
        "\n",
        "역전파\n",
        "최적화 알고리즘\n",
        "경사 하강법\n",
        "\n",
        "배치 경사 하강법: 전체 데이터 셋을 사용, 수렴 과정이 안정적, 계산 비용이 높고 메모리 사용량 많음\n",
        "확률적 경사 하강법: 하나의 데이터 포인트를 사용, 계산이 빠르고 메모리 사용량 적음, 수렴 과정에서 진동 가능성 있음\n",
        "미니 배치 경사 하강법: 전체 데이터 셋을 작은 배치로 나누어 사용, 계산 효율과 메모리 사용량의 균형을 잡을 수 있다, 수렴 과정이 비교적 안정적, 배치 크기 선택이 어려울 수 있으며 하이터파라미터 튜닝 필요\n",
        "\n",
        "배치 크기가 크면 안정적이지만 계산이 느림, 배치 크기가 작으면 계산이 빠르지만 안정적이지 않음\n",
        "학습률이 너무 크면 최적점 주변에서 진동, 너무 작으면 최적점에 도달하기까지 시간이 오래 걸림\n",
        "\n",
        "역전파의 한계: 계산 비용, 기울기 소실, 기울기 폭발\n",
        "\n",
        "- 딥러닝의 과적합과 해결방법\n",
        "모델이 복잡하고 데이터 셋이 크기 때문에 과적합 가능성이 높다. -> 데이터 증강, 정규화, 드롭아웃\n",
        "\n",
        "\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#실습"
      ],
      "metadata": {
        "id": "MZwiknbgl7jL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cuid3Pbzpvp1"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터셋 불러오기 및 전처리\n",
        "transform = transforms.Compose([transforms.ToTensor(),  # 데이터를 텐서로 변환\n",
        "                                transforms.Normalize((0,), (1,))])  # 데이터를 0의 평균과 1의 표준편차로 정규화"
      ],
      "metadata": {
        "id": "WEnpAm4wp6hV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 훈련 데이터셋과 테스트 데이터셋 다운로드 및 로드\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "81bcBsvYp9G0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터로더를 사용하여 데이터셋을 배치 단위로 로드\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)"
      ],
      "metadata": {
        "id": "A6ZvqNsvp9Eh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "xrQ3fog_v2Rh",
        "outputId": "88ad69d6-e243-4ad4-e86b-e44cfa355e30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0118, 0.0706, 0.0706, 0.0706,\n",
              "           0.4941, 0.5333, 0.6863, 0.1020, 0.6510, 1.0000, 0.9686, 0.4980,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.1176, 0.1412, 0.3686, 0.6039, 0.6667, 0.9922, 0.9922, 0.9922,\n",
              "           0.9922, 0.9922, 0.8824, 0.6745, 0.9922, 0.9490, 0.7647, 0.2510,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1922,\n",
              "           0.9333, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922,\n",
              "           0.9922, 0.9843, 0.3647, 0.3216, 0.3216, 0.2196, 0.1529, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0706,\n",
              "           0.8588, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.7765, 0.7137,\n",
              "           0.9686, 0.9451, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.3137, 0.6118, 0.4196, 0.9922, 0.9922, 0.8039, 0.0431, 0.0000,\n",
              "           0.1686, 0.6039, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0549, 0.0039, 0.6039, 0.9922, 0.3529, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.5451, 0.9922, 0.7451, 0.0078, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0431, 0.7451, 0.9922, 0.2745, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.1373, 0.9451, 0.8824, 0.6275,\n",
              "           0.4235, 0.0039, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3176, 0.9412, 0.9922,\n",
              "           0.9922, 0.4667, 0.0980, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1765, 0.7294,\n",
              "           0.9922, 0.9922, 0.5882, 0.1059, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0627,\n",
              "           0.3647, 0.9882, 0.9922, 0.7333, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.9765, 0.9922, 0.9765, 0.2510, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1804, 0.5098,\n",
              "           0.7176, 0.9922, 0.9922, 0.8118, 0.0078, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.1529, 0.5804, 0.8980, 0.9922,\n",
              "           0.9922, 0.9922, 0.9804, 0.7137, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0941, 0.4471, 0.8667, 0.9922, 0.9922, 0.9922,\n",
              "           0.9922, 0.7882, 0.3059, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0902, 0.2588, 0.8353, 0.9922, 0.9922, 0.9922, 0.9922, 0.7765,\n",
              "           0.3176, 0.0078, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0706, 0.6706,\n",
              "           0.8588, 0.9922, 0.9922, 0.9922, 0.9922, 0.7647, 0.3137, 0.0353,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.2157, 0.6745, 0.8863, 0.9922,\n",
              "           0.9922, 0.9922, 0.9922, 0.9569, 0.5216, 0.0431, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.5333, 0.9922, 0.9922, 0.9922,\n",
              "           0.8314, 0.5294, 0.5176, 0.0627, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000]]]),\n",
              " 5)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 신경망 모델 정의\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(28 * 28, 128)  # 입력층: 28x28 이미지를 1차원 배열로 변환, 128개의 뉴런\n",
        "        self.fc2 = nn.Linear(128, 10)  # 출력층: 10개의 뉴런 (0~9 클래스)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28 * 28)  # 이미지를 1차원 배열로 변환\n",
        "        x = F.relu(self.fc1(x))  # 은닉층: ReLU 활성화 함수 적용\n",
        "        x = self.fc2(x)  # 출력층\n",
        "        return F.log_softmax(x, dim=1)  # 소프트맥스 함수로 클래스 확률 반환"
      ],
      "metadata": {
        "id": "I6HEIgwIp9CG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 초기화\n",
        "model = SimpleNN()\n",
        "\n",
        "# 손실 함수와 옵티마이저 정의\n",
        "criterion = nn.CrossEntropyLoss()  # 교차 엔트로피 손실 함수\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)  # SGD 옵티마이저"
      ],
      "metadata": {
        "id": "zMV0MZt0p8_S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 학습 함수 정의\n",
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "    model.train()  # 모델을 학습 모드로 설정\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)  # 데이터를 장치로 이동\n",
        "        optimizer.zero_grad()  # 이전 기울기 초기화\n",
        "        output = model(data)  # 모델 예측\n",
        "        loss = criterion(output, target)  # 손실 계산\n",
        "        loss.backward()  # 역전파를 통해 기울기 계산\n",
        "        optimizer.step()  # 가중치 업데이트\n",
        "        if batch_idx % 100 == 0:  # 100번째 배치마다 로그 출력\n",
        "            print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} '\n",
        "                  f'({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}')\n",
        "\n"
      ],
      "metadata": {
        "id": "y3I-czoNqM-M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 평가 함수 정의\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()  # 모델을 평가 모드로 설정\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():  # 평가 시에는 기울기를 계산하지 않음\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += criterion(output, target).item()  # 손실 합산\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # 가장 높은 확률을 가진 클래스 예측\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()  # 맞춘 개수 합산\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    test_accuracy = 100. * correct / len(test_loader.dataset)\n",
        "    print(f'\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} '\n",
        "          f'({test_accuracy:.0f}%)\\n')\n",
        "    return test_loss, test_accuracy"
      ],
      "metadata": {
        "id": "e1JuCnrtqPIA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습 및 평가\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "epochs = 3\n",
        "train_losses, test_losses, test_accuracies = [], [], []"
      ],
      "metadata": {
        "id": "WrGAXerxqQ_P",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "outputId": "2fdf2e25-7b67-4fae-f6cd-9fde20fca2ff"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-0417daf8c31a>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 학습 및 평가\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 학습\n",
        "for epoch in range(1, epochs + 1):\n",
        "    train(model, device, train_loader, optimizer, epoch)\n",
        "    test_loss, test_accuracy = test(model, device, test_loader)\n",
        "    test_losses.append(test_loss)\n",
        "    test_accuracies.append(test_accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HDHYY6QVrDbZ",
        "outputId": "e1664c0b-75d7-42b1-c02a-e0474ffc7119"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.320083\n",
            "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.442556\n",
            "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.313872\n",
            "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.286916\n",
            "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.346181\n",
            "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.286633\n",
            "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.229845\n",
            "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.156307\n",
            "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.264982\n",
            "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.356415\n",
            "\n",
            "Test set: Average loss: 0.0002, Accuracy: 9337/10000 (93%)\n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.218675\n",
            "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.253333\n",
            "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.290704\n",
            "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.105914\n",
            "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.229578\n",
            "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.144221\n",
            "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.150911\n",
            "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.102547\n",
            "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.050327\n",
            "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.100468\n",
            "\n",
            "Test set: Average loss: 0.0002, Accuracy: 9548/10000 (95%)\n",
            "\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.200064\n",
            "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.178990\n",
            "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.045495\n",
            "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.294723\n",
            "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.110199\n",
            "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.122125\n",
            "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.161718\n",
            "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.248488\n",
            "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.142302\n",
            "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.105349\n",
            "\n",
            "Test set: Average loss: 0.0001, Accuracy: 9604/10000 (96%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchsummary import summary\n",
        "summary(model, input_size=(1, 28, 28))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xOupew0rqSVB",
        "outputId": "c0f44bde-1f5e-4f58-c3c7-c4eb11daee9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Linear-1                  [-1, 128]         100,480\n",
            "            Linear-2                   [-1, 10]           1,290\n",
            "================================================================\n",
            "Total params: 101,770\n",
            "Trainable params: 101,770\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.00\n",
            "Params size (MB): 0.39\n",
            "Estimated Total Size (MB): 0.39\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# 시드 고정\n",
        "def set_seed(seed):\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "\n",
        "\n",
        "# ReLU-y 활성화 함수 정의\n",
        "class ReLU_Y(nn.Module):\n",
        "    def forward(self, x):\n",
        "        return torch.min(torch.zeros_like(x), x)\n",
        "\n",
        "# MNIST 데이터셋 로드 및 전처리\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
        "trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
        "testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=1000, shuffle=False)\n",
        "\n",
        "# ReLU를 사용하는 간단한 신경망 정의\n",
        "class NetReLU(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NetReLU, self).__init__()\n",
        "        self.fc1 = nn.Linear(28*28, 512)\n",
        "        self.fc2 = nn.Linear(512, 512)\n",
        "        self.fc3 = nn.Linear(512, 10)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28*28)  # Flatten the input\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# ReLU-y를 사용하는 간단한 신경망 정의\n",
        "class NetReLU_Y(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NetReLU_Y, self).__init__()\n",
        "        self.fc1 = nn.Linear(28*28, 512)\n",
        "        self.fc2 = nn.Linear(512, 512)\n",
        "        self.fc3 = nn.Linear(512, 10)\n",
        "        self.relu_y = ReLU_Y()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28*28)  # Flatten the input\n",
        "        x = self.relu_y(self.fc1(x))\n",
        "        x = self.relu_y(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# 모델 학습 함수\n",
        "def train_model(model, trainloader, criterion, optimizer, epochs=5):\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "        for inputs, labels in trainloader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "        print(f\"Epoch {epoch+1}, Loss: {running_loss/len(trainloader)}\")\n",
        "\n",
        "# 모델 평가 함수\n",
        "def test_model(model, testloader):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in testloader:\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    print(f'Accuracy: {100 * correct / total}%')\n",
        "\n",
        "# 모델 초기화\n",
        "net_relu = NetReLU()\n",
        "net_relu_y = NetReLU_Y()\n",
        "\n",
        "with torch.no_grad():\n",
        "    for param_relu, param_relu_y in zip(net_relu.parameters(), net_relu_y.parameters()):\n",
        "        param_relu_y.data = -torch.abs(param_relu.data) * torch.sign(param_relu.data)\n",
        "\n",
        "\n",
        "# 손실 함수와 옵티마이저 정의\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer_relu = optim.SGD(net_relu.parameters(), lr=0.01, momentum=0.9)\n",
        "optimizer_relu_y = optim.SGD(net_relu_y.parameters(), lr=0.01, momentum=0.9)\n",
        "\n",
        "\n",
        "# ReLU 네트워크 학습\n",
        "print(\"Training ReLU network...\")\n",
        "train_model(net_relu, trainloader, criterion, optimizer_relu, epochs=5)\n",
        "\n",
        "# ReLU-y 네트워크 학습\n",
        "print(\"Training ReLU-y network...\")\n",
        "train_model(net_relu_y, trainloader, criterion, optimizer_relu_y, epochs=5)\n",
        "\n",
        "# ReLU 네트워크 평가\n",
        "print(\"Testing ReLU network...\")\n",
        "test_model(net_relu, testloader)\n",
        "\n",
        "# ReLU-y 네트워크 평가\n",
        "print(\"Testing ReLU-y network...\")\n",
        "test_model(net_relu_y, testloader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cm1qGYjKzN7N",
        "outputId": "958d67d9-79e2-4801-b91e-c61b245287eb"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training ReLU network...\n",
            "Epoch 1, Loss: 0.38039735969958277\n",
            "Epoch 2, Loss: 0.1529672586881339\n",
            "Epoch 3, Loss: 0.10826150237469435\n",
            "Epoch 4, Loss: 0.08443819391412108\n",
            "Epoch 5, Loss: 0.06907687925506256\n",
            "Training ReLU-y network...\n",
            "Epoch 1, Loss: 0.388705630435237\n",
            "Epoch 2, Loss: 0.15379621476026328\n",
            "Epoch 3, Loss: 0.10626411362231444\n",
            "Epoch 4, Loss: 0.08456459207468227\n",
            "Epoch 5, Loss: 0.06752357331699313\n",
            "Testing ReLU network...\n",
            "Accuracy: 97.57%\n",
            "Testing ReLU-y network...\n",
            "Accuracy: 97.11%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 파라미터의 부호 비교 함수\n",
        "def compare_parameter_signs(model1, model2):\n",
        "    params1 = model1.named_parameters()\n",
        "    params2 = model2.named_parameters()\n",
        "\n",
        "    dict_params2 = dict(params2)\n",
        "    total_param_count = 0\n",
        "    different_sign_count = 0\n",
        "\n",
        "    for name1, param1 in params1:\n",
        "        if name1 in dict_params2:\n",
        "            param2 = dict_params2[name1]\n",
        "            sign1 = torch.sign(param1)\n",
        "            sign2 = torch.sign(param2)\n",
        "            total_param_count += param1.numel()  # 전체 파라미터 개수 추가\n",
        "            if not torch.equal(sign1, sign2):\n",
        "                different_sign_count += torch.sum(sign1 != sign2).item()  # 부호가 다른 파라미터 개수 추가\n",
        "                print(f\"Parameter {name1} has different signs.\")\n",
        "                print(f\"Model 1 - {name1}: {sign1}\")\n",
        "                print(f\"Model 2 - {name1}: {sign2}\")\n",
        "\n",
        "    print(f\"Total number of parameters: {total_param_count}\")\n",
        "    print(f\"Total number of parameters with different signs: {different_sign_count}\")\n",
        "\n",
        "print(\"\\nComparing parameter signs between ReLU and ReLU-y networks...\")\n",
        "compare_parameter_signs(net_relu, net_relu_y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eB4ffmNm1NS9",
        "outputId": "a845c54d-060f-4409-a6bc-4ec6a8e70e04"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Comparing parameter signs between ReLU and ReLU-y networks...\n",
            "Parameter fc1.weight has different signs.\n",
            "Model 1 - fc1.weight: tensor([[ 1.,  1., -1.,  ..., -1.,  1.,  1.],\n",
            "        [-1., -1.,  1.,  ..., -1., -1.,  1.],\n",
            "        [ 1.,  1.,  1.,  ..., -1., -1.,  1.],\n",
            "        ...,\n",
            "        [-1., -1.,  1.,  ...,  1.,  1., -1.],\n",
            "        [ 1.,  1.,  1.,  ..., -1., -1.,  1.],\n",
            "        [-1.,  1., -1.,  ...,  1., -1., -1.]], grad_fn=<SignBackward0>)\n",
            "Model 2 - fc1.weight: tensor([[-1., -1.,  1.,  ...,  1., -1., -1.],\n",
            "        [ 1.,  1., -1.,  ...,  1., -1., -1.],\n",
            "        [-1., -1., -1.,  ...,  1.,  1., -1.],\n",
            "        ...,\n",
            "        [ 1.,  1., -1.,  ..., -1., -1.,  1.],\n",
            "        [-1., -1., -1.,  ...,  1.,  1., -1.],\n",
            "        [ 1., -1.,  1.,  ..., -1.,  1.,  1.]], grad_fn=<SignBackward0>)\n",
            "Parameter fc1.bias has different signs.\n",
            "Model 1 - fc1.bias: tensor([-1., -1., -1.,  1.,  1.,  1.,  1.,  1.,  1., -1.,  1.,  1.,  1.,  1.,\n",
            "         1.,  1., -1.,  1., -1.,  1., -1., -1.,  1., -1., -1.,  1.,  1., -1.,\n",
            "         1.,  1., -1.,  1., -1., -1.,  1., -1.,  1.,  1.,  1., -1., -1., -1.,\n",
            "         1.,  1., -1.,  1., -1.,  1., -1.,  1.,  1.,  1., -1.,  1., -1., -1.,\n",
            "         1.,  1., -1.,  1., -1.,  1., -1.,  1.,  1., -1.,  1., -1.,  1., -1.,\n",
            "        -1.,  1., -1., -1.,  1.,  1.,  1., -1.,  1.,  1., -1., -1., -1., -1.,\n",
            "        -1.,  1.,  1., -1., -1.,  1.,  1.,  1., -1.,  1., -1., -1., -1.,  1.,\n",
            "         1., -1., -1.,  1., -1.,  1.,  1., -1.,  1., -1.,  1.,  1., -1.,  1.,\n",
            "        -1.,  1.,  1., -1., -1., -1., -1.,  1.,  1.,  1., -1.,  1.,  1., -1.,\n",
            "         1., -1.,  1.,  1.,  1.,  1.,  1., -1., -1.,  1., -1., -1., -1., -1.,\n",
            "        -1.,  1.,  1.,  1., -1., -1., -1.,  1.,  1., -1., -1., -1.,  1.,  1.,\n",
            "        -1., -1., -1.,  1.,  1., -1.,  1.,  1., -1.,  1.,  1., -1., -1., -1.,\n",
            "        -1., -1.,  1.,  1., -1.,  1.,  1., -1.,  1., -1.,  1., -1.,  1.,  1.,\n",
            "        -1., -1., -1., -1.,  1.,  1., -1.,  1., -1., -1., -1., -1., -1.,  1.,\n",
            "         1.,  1., -1.,  1.,  1.,  1., -1., -1., -1., -1.,  1., -1.,  1., -1.,\n",
            "        -1., -1., -1., -1., -1., -1., -1., -1., -1.,  1.,  1.,  1., -1.,  1.,\n",
            "        -1.,  1.,  1.,  1., -1., -1.,  1.,  1.,  1., -1., -1.,  1.,  1.,  1.,\n",
            "         1.,  1., -1.,  1.,  1., -1.,  1., -1.,  1., -1., -1.,  1., -1.,  1.,\n",
            "         1., -1.,  1., -1., -1., -1., -1., -1., -1., -1.,  1.,  1., -1., -1.,\n",
            "         1.,  1.,  1., -1.,  1., -1., -1., -1.,  1.,  1.,  1.,  1., -1.,  1.,\n",
            "         1.,  1.,  1., -1.,  1.,  1.,  1., -1.,  1.,  1.,  1.,  1.,  1., -1.,\n",
            "        -1.,  1.,  1., -1., -1.,  1.,  1., -1.,  1.,  1., -1.,  1.,  1., -1.,\n",
            "         1., -1., -1., -1.,  1.,  1., -1.,  1., -1., -1., -1., -1., -1., -1.,\n",
            "        -1.,  1., -1., -1.,  1.,  1.,  1.,  1., -1.,  1., -1.,  1., -1.,  1.,\n",
            "        -1.,  1., -1.,  1.,  1., -1., -1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
            "         1., -1.,  1., -1., -1., -1.,  1., -1., -1.,  1., -1., -1., -1., -1.,\n",
            "        -1.,  1., -1., -1., -1.,  1.,  1., -1.,  1.,  1., -1.,  1.,  1.,  1.,\n",
            "        -1., -1.,  1., -1., -1., -1.,  1., -1.,  1.,  1., -1.,  1.,  1., -1.,\n",
            "         1.,  1.,  1.,  1., -1., -1., -1., -1., -1.,  1., -1., -1.,  1., -1.,\n",
            "        -1.,  1., -1., -1.,  1.,  1.,  1., -1., -1., -1.,  1., -1.,  1.,  1.,\n",
            "        -1.,  1., -1., -1.,  1.,  1.,  1.,  1., -1.,  1., -1.,  1., -1., -1.,\n",
            "        -1.,  1.,  1.,  1.,  1., -1.,  1., -1., -1., -1., -1., -1.,  1., -1.,\n",
            "        -1.,  1., -1.,  1.,  1.,  1., -1.,  1., -1.,  1., -1., -1.,  1., -1.,\n",
            "        -1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1., -1.,  1., -1., -1.,  1.,\n",
            "        -1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1., -1.,  1., -1.,  1., -1.,\n",
            "         1.,  1., -1., -1.,  1.,  1.,  1., -1.,  1.,  1., -1., -1.,  1.,  1.,\n",
            "         1., -1., -1.,  1.,  1., -1.,  1., -1.], grad_fn=<SignBackward0>)\n",
            "Model 2 - fc1.bias: tensor([ 1.,  1.,  1., -1., -1., -1., -1., -1., -1.,  1., -1., -1., -1., -1.,\n",
            "        -1., -1.,  1., -1., -1., -1.,  1.,  1., -1.,  1.,  1., -1., -1.,  1.,\n",
            "         1., -1.,  1., -1.,  1.,  1., -1.,  1., -1., -1., -1.,  1.,  1.,  1.,\n",
            "        -1., -1.,  1., -1.,  1., -1.,  1., -1., -1., -1.,  1., -1.,  1.,  1.,\n",
            "        -1., -1.,  1., -1.,  1., -1.,  1., -1., -1.,  1., -1.,  1., -1.,  1.,\n",
            "         1., -1.,  1.,  1., -1., -1., -1.,  1., -1., -1.,  1.,  1.,  1.,  1.,\n",
            "         1., -1., -1.,  1.,  1., -1., -1., -1.,  1., -1.,  1.,  1.,  1., -1.,\n",
            "        -1.,  1.,  1., -1.,  1., -1., -1.,  1., -1.,  1., -1., -1.,  1., -1.,\n",
            "         1., -1., -1.,  1.,  1.,  1.,  1., -1., -1., -1.,  1., -1., -1.,  1.,\n",
            "        -1.,  1., -1., -1., -1., -1., -1.,  1.,  1., -1.,  1.,  1.,  1.,  1.,\n",
            "         1., -1., -1., -1.,  1.,  1.,  1.,  1., -1.,  1.,  1.,  1., -1., -1.,\n",
            "         1.,  1.,  1.,  1., -1.,  1., -1., -1.,  1.,  1., -1.,  1.,  1.,  1.,\n",
            "         1.,  1., -1., -1.,  1., -1., -1.,  1., -1.,  1., -1.,  1., -1., -1.,\n",
            "         1.,  1.,  1.,  1., -1., -1.,  1., -1.,  1.,  1., -1.,  1.,  1., -1.,\n",
            "        -1., -1.,  1., -1., -1., -1.,  1.,  1.,  1.,  1., -1.,  1., -1.,  1.,\n",
            "         1.,  1.,  1., -1.,  1.,  1.,  1.,  1.,  1., -1., -1., -1.,  1.,  1.,\n",
            "         1., -1., -1., -1.,  1.,  1., -1., -1.,  1.,  1.,  1., -1., -1., -1.,\n",
            "        -1., -1.,  1., -1., -1.,  1., -1.,  1., -1.,  1.,  1., -1.,  1., -1.,\n",
            "        -1.,  1., -1.,  1.,  1.,  1.,  1.,  1.,  1.,  1., -1., -1.,  1.,  1.,\n",
            "        -1., -1., -1.,  1., -1.,  1.,  1.,  1., -1., -1., -1., -1.,  1., -1.,\n",
            "        -1., -1., -1.,  1., -1., -1., -1.,  1., -1., -1., -1., -1., -1.,  1.,\n",
            "        -1., -1., -1.,  1.,  1., -1., -1.,  1., -1., -1.,  1., -1., -1.,  1.,\n",
            "        -1.,  1.,  1.,  1., -1., -1.,  1., -1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
            "         1., -1.,  1.,  1., -1., -1., -1., -1.,  1., -1.,  1., -1.,  1., -1.,\n",
            "         1., -1.,  1., -1., -1.,  1.,  1., -1., -1., -1., -1., -1., -1., -1.,\n",
            "        -1.,  1., -1.,  1.,  1.,  1., -1.,  1., -1., -1.,  1.,  1.,  1.,  1.,\n",
            "         1., -1.,  1.,  1.,  1., -1., -1.,  1., -1., -1.,  1., -1., -1.,  1.,\n",
            "         1.,  1., -1.,  1.,  1.,  1., -1.,  1., -1., -1.,  1., -1., -1., -1.,\n",
            "        -1., -1., -1., -1.,  1.,  1.,  1.,  1.,  1., -1.,  1.,  1., -1.,  1.,\n",
            "         1., -1.,  1.,  1., -1., -1., -1.,  1.,  1.,  1., -1.,  1., -1.,  1.,\n",
            "         1., -1.,  1.,  1., -1., -1., -1., -1.,  1., -1.,  1., -1.,  1.,  1.,\n",
            "         1., -1., -1., -1., -1.,  1., -1.,  1.,  1.,  1.,  1.,  1., -1.,  1.,\n",
            "         1., -1.,  1., -1., -1., -1.,  1., -1.,  1., -1.,  1.,  1., -1.,  1.,\n",
            "         1., -1., -1., -1., -1., -1., -1., -1., -1.,  1., -1.,  1.,  1., -1.,\n",
            "         1., -1., -1., -1., -1., -1., -1., -1., -1.,  1., -1.,  1., -1.,  1.,\n",
            "        -1., -1.,  1.,  1., -1., -1., -1.,  1., -1., -1.,  1.,  1., -1., -1.,\n",
            "        -1.,  1.,  1., -1., -1.,  1., -1.,  1.], grad_fn=<SignBackward0>)\n",
            "Parameter fc2.weight has different signs.\n",
            "Model 1 - fc2.weight: tensor([[ 1.,  1., -1.,  ...,  1., -1.,  1.],\n",
            "        [ 1.,  1., -1.,  ..., -1., -1.,  1.],\n",
            "        [-1.,  1., -1.,  ...,  1., -1., -1.],\n",
            "        ...,\n",
            "        [ 1., -1.,  1.,  ..., -1.,  1., -1.],\n",
            "        [-1., -1.,  1.,  ...,  1., -1., -1.],\n",
            "        [ 1.,  1., -1.,  ...,  1.,  1., -1.]], grad_fn=<SignBackward0>)\n",
            "Model 2 - fc2.weight: tensor([[-1., -1.,  1.,  ..., -1.,  1., -1.],\n",
            "        [-1., -1.,  1.,  ...,  1.,  1.,  1.],\n",
            "        [ 1., -1., -1.,  ..., -1.,  1.,  1.],\n",
            "        ...,\n",
            "        [-1.,  1., -1.,  ...,  1., -1.,  1.],\n",
            "        [ 1.,  1., -1.,  ..., -1.,  1., -1.],\n",
            "        [-1., -1.,  1.,  ...,  1., -1.,  1.]], grad_fn=<SignBackward0>)\n",
            "Parameter fc2.bias has different signs.\n",
            "Model 1 - fc2.bias: tensor([ 1., -1.,  1.,  1., -1., -1.,  1.,  1.,  1.,  1.,  1.,  1., -1., -1.,\n",
            "         1.,  1.,  1., -1.,  1.,  1., -1., -1.,  1., -1.,  1.,  1.,  1.,  1.,\n",
            "        -1., -1.,  1.,  1., -1., -1.,  1., -1., -1., -1.,  1.,  1.,  1., -1.,\n",
            "         1.,  1., -1.,  1.,  1., -1.,  1.,  1.,  1.,  1., -1.,  1., -1.,  1.,\n",
            "         1., -1., -1.,  1.,  1., -1.,  1., -1.,  1., -1., -1.,  1., -1., -1.,\n",
            "         1., -1.,  1.,  1., -1.,  1.,  1., -1., -1.,  1., -1.,  1.,  1., -1.,\n",
            "        -1.,  1.,  1., -1., -1., -1., -1.,  1., -1., -1.,  1.,  1.,  1., -1.,\n",
            "         1.,  1., -1., -1., -1.,  1.,  1.,  1.,  1.,  1., -1.,  1., -1., -1.,\n",
            "         1., -1., -1., -1.,  1., -1., -1.,  1.,  1., -1., -1.,  1., -1.,  1.,\n",
            "         1., -1.,  1., -1.,  1., -1., -1.,  1., -1., -1., -1.,  1.,  1.,  1.,\n",
            "         1., -1.,  1.,  1., -1., -1., -1.,  1.,  1.,  1., -1., -1.,  1., -1.,\n",
            "        -1.,  1., -1.,  1., -1., -1., -1.,  1., -1.,  1.,  1.,  1.,  1., -1.,\n",
            "        -1., -1.,  1., -1.,  1.,  1., -1.,  1.,  1., -1., -1.,  1., -1.,  1.,\n",
            "        -1.,  1., -1., -1.,  1.,  1.,  1., -1., -1.,  1., -1., -1., -1., -1.,\n",
            "        -1.,  1., -1., -1., -1.,  1., -1., -1.,  1.,  1., -1., -1.,  1., -1.,\n",
            "        -1.,  1., -1., -1., -1., -1.,  1., -1., -1.,  1.,  1.,  1., -1., -1.,\n",
            "         1., -1.,  1., -1., -1., -1., -1.,  1.,  1., -1.,  1., -1.,  1.,  1.,\n",
            "         1.,  1.,  1.,  1.,  1., -1., -1., -1., -1., -1.,  1., -1.,  1., -1.,\n",
            "         1.,  1.,  1.,  1.,  1.,  1.,  1., -1.,  1.,  1.,  1.,  1.,  1., -1.,\n",
            "         1.,  1., -1.,  1., -1., -1., -1.,  1., -1.,  1., -1.,  1., -1.,  1.,\n",
            "        -1.,  1.,  1.,  1., -1.,  1.,  1., -1., -1.,  1., -1.,  1., -1.,  1.,\n",
            "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
            "        -1.,  1., -1., -1., -1., -1.,  1.,  1.,  1.,  1., -1.,  1.,  1., -1.,\n",
            "        -1.,  1.,  1., -1., -1., -1.,  1.,  1., -1.,  1., -1., -1.,  1., -1.,\n",
            "        -1., -1.,  1., -1.,  1., -1.,  1.,  1.,  1.,  1., -1.,  1.,  1., -1.,\n",
            "        -1., -1., -1.,  1., -1.,  1.,  1.,  1., -1.,  1., -1., -1.,  1., -1.,\n",
            "        -1.,  1., -1., -1.,  1.,  1.,  1., -1., -1., -1.,  1.,  1.,  1., -1.,\n",
            "         1., -1.,  1., -1., -1., -1., -1., -1.,  1.,  1.,  1.,  1., -1.,  1.,\n",
            "        -1.,  1.,  1.,  1.,  1.,  1., -1., -1.,  1., -1., -1., -1., -1., -1.,\n",
            "        -1., -1., -1., -1.,  1., -1., -1.,  1., -1.,  1.,  1.,  1.,  1.,  1.,\n",
            "        -1., -1.,  1., -1., -1., -1., -1., -1.,  1., -1., -1., -1., -1., -1.,\n",
            "         1., -1.,  1.,  1.,  1.,  1.,  1.,  1.,  1., -1., -1., -1.,  1.,  1.,\n",
            "         1., -1.,  1., -1., -1.,  1.,  1., -1.,  1.,  1.,  1.,  1., -1., -1.,\n",
            "        -1., -1.,  1.,  1.,  1., -1., -1., -1.,  1., -1.,  1.,  1., -1.,  1.,\n",
            "        -1.,  1.,  1., -1.,  1.,  1.,  1., -1., -1.,  1., -1., -1.,  1.,  1.,\n",
            "        -1., -1., -1.,  1., -1.,  1., -1.,  1., -1.,  1.,  1.,  1.,  1.,  1.,\n",
            "         1.,  1.,  1., -1.,  1., -1.,  1., -1.], grad_fn=<SignBackward0>)\n",
            "Model 2 - fc2.bias: tensor([-1.,  1., -1., -1.,  1.,  1., -1., -1., -1., -1., -1., -1.,  1.,  1.,\n",
            "        -1., -1., -1.,  1., -1., -1.,  1.,  1., -1.,  1., -1., -1.,  1., -1.,\n",
            "         1.,  1.,  1., -1.,  1.,  1., -1.,  1.,  1.,  1., -1., -1.,  1.,  1.,\n",
            "        -1., -1.,  1., -1., -1.,  1., -1., -1., -1., -1.,  1., -1.,  1., -1.,\n",
            "         1.,  1.,  1., -1., -1.,  1.,  1.,  1., -1.,  1.,  1.,  1.,  1.,  1.,\n",
            "        -1.,  1., -1., -1.,  1.,  1., -1.,  1.,  1., -1.,  1., -1., -1.,  1.,\n",
            "         1., -1., -1.,  1.,  1.,  1., -1., -1., -1.,  1.,  1., -1., -1.,  1.,\n",
            "        -1., -1.,  1.,  1.,  1., -1., -1., -1.,  1.,  1., -1., -1.,  1.,  1.,\n",
            "        -1., -1.,  1., -1., -1., -1.,  1., -1., -1.,  1., -1., -1., -1., -1.,\n",
            "        -1.,  1., -1.,  1., -1., -1.,  1., -1.,  1.,  1.,  1., -1., -1., -1.,\n",
            "        -1.,  1., -1., -1.,  1.,  1.,  1., -1.,  1., -1., -1.,  1., -1.,  1.,\n",
            "         1., -1.,  1., -1.,  1.,  1.,  1., -1.,  1., -1., -1., -1., -1.,  1.,\n",
            "         1.,  1., -1.,  1., -1., -1.,  1., -1.,  1.,  1.,  1., -1.,  1., -1.,\n",
            "         1., -1.,  1.,  1., -1., -1., -1.,  1.,  1., -1.,  1.,  1.,  1.,  1.,\n",
            "         1., -1.,  1.,  1.,  1., -1., -1.,  1., -1., -1.,  1.,  1., -1., -1.,\n",
            "        -1., -1., -1.,  1.,  1.,  1., -1.,  1.,  1., -1., -1., -1.,  1.,  1.,\n",
            "        -1., -1., -1.,  1.,  1.,  1.,  1., -1., -1.,  1.,  1.,  1., -1., -1.,\n",
            "        -1., -1., -1.,  1., -1.,  1.,  1.,  1.,  1., -1., -1.,  1., -1.,  1.,\n",
            "        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,  1.,\n",
            "        -1., -1.,  1., -1.,  1.,  1.,  1., -1.,  1., -1.,  1., -1.,  1., -1.,\n",
            "         1.,  1., -1., -1.,  1., -1., -1.,  1., -1., -1., -1., -1.,  1., -1.,\n",
            "        -1., -1., -1., -1., -1., -1., -1., -1.,  1., -1., -1., -1., -1.,  1.,\n",
            "         1., -1.,  1.,  1.,  1.,  1., -1., -1., -1., -1.,  1., -1., -1.,  1.,\n",
            "         1., -1.,  1.,  1.,  1.,  1., -1., -1.,  1., -1.,  1.,  1., -1.,  1.,\n",
            "         1., -1.,  1.,  1., -1.,  1., -1., -1.,  1., -1.,  1., -1., -1.,  1.,\n",
            "         1.,  1.,  1., -1.,  1., -1., -1., -1.,  1., -1.,  1.,  1., -1.,  1.,\n",
            "         1.,  1.,  1.,  1., -1.,  1.,  1.,  1.,  1.,  1., -1., -1., -1.,  1.,\n",
            "        -1.,  1., -1.,  1.,  1.,  1.,  1.,  1., -1., -1., -1., -1.,  1., -1.,\n",
            "         1., -1., -1., -1., -1., -1.,  1.,  1., -1.,  1.,  1.,  1., -1.,  1.,\n",
            "         1.,  1.,  1.,  1., -1.,  1.,  1., -1.,  1., -1., -1., -1., -1., -1.,\n",
            "         1.,  1., -1.,  1.,  1., -1.,  1.,  1., -1.,  1.,  1.,  1.,  1.,  1.,\n",
            "        -1.,  1., -1., -1., -1.,  1., -1., -1., -1.,  1.,  1.,  1., -1., -1.,\n",
            "        -1., -1., -1.,  1.,  1., -1., -1.,  1., -1., -1., -1., -1.,  1.,  1.,\n",
            "         1.,  1., -1., -1., -1.,  1.,  1.,  1., -1.,  1., -1., -1.,  1., -1.,\n",
            "         1., -1.,  1.,  1., -1., -1., -1.,  1.,  1., -1.,  1.,  1., -1., -1.,\n",
            "         1.,  1.,  1., -1.,  1., -1.,  1.,  1.,  1., -1., -1., -1., -1., -1.,\n",
            "        -1., -1., -1.,  1., -1.,  1., -1.,  1.], grad_fn=<SignBackward0>)\n",
            "Parameter fc3.weight has different signs.\n",
            "Model 1 - fc3.weight: tensor([[-1., -1.,  1.,  ..., -1.,  1.,  1.],\n",
            "        [ 1., -1., -1.,  ..., -1.,  1., -1.],\n",
            "        [-1.,  1.,  1.,  ...,  1.,  1.,  1.],\n",
            "        ...,\n",
            "        [-1.,  1.,  1.,  ...,  1.,  1.,  1.],\n",
            "        [ 1., -1., -1.,  ..., -1., -1., -1.],\n",
            "        [-1.,  1.,  1.,  ..., -1., -1., -1.]], grad_fn=<SignBackward0>)\n",
            "Model 2 - fc3.weight: tensor([[ 1.,  1., -1.,  ..., -1.,  1.,  1.],\n",
            "        [ 1.,  1.,  1.,  ...,  1., -1.,  1.],\n",
            "        [ 1.,  1.,  1.,  ..., -1., -1., -1.],\n",
            "        ...,\n",
            "        [ 1.,  1.,  1.,  ..., -1., -1., -1.],\n",
            "        [-1.,  1.,  1.,  ...,  1., -1.,  1.],\n",
            "        [-1., -1., -1.,  ..., -1.,  1.,  1.]], grad_fn=<SignBackward0>)\n",
            "Parameter fc3.bias has different signs.\n",
            "Model 1 - fc3.bias: tensor([-1., -1., -1.,  1.,  1., -1., -1., -1.,  1.,  1.],\n",
            "       grad_fn=<SignBackward0>)\n",
            "Model 2 - fc3.bias: tensor([-1., -1., -1., -1.,  1.,  1., -1., -1.,  1.,  1.],\n",
            "       grad_fn=<SignBackward0>)\n",
            "Total number of parameters: 669706\n",
            "Total number of parameters with different signs: 586632\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# 시드 고정\n",
        "def set_seed(seed):\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "# ReLU-y 활성화 함수 정의\n",
        "class ReLU_Y(nn.Module):\n",
        "    def forward(self, x):\n",
        "        return torch.min(torch.zeros_like(x), x)\n",
        "\n",
        "# MNIST 데이터셋 로드 및 전처리\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
        "trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
        "testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=1000, shuffle=False)\n",
        "\n",
        "# ReLU를 사용하는 간단한 신경망 정의\n",
        "class NetReLU(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NetReLU, self).__init__()\n",
        "        self.fc1 = nn.Linear(28*28, 512)\n",
        "        self.fc2 = nn.Linear(512, 512)\n",
        "        self.fc3 = nn.Linear(512, 10)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28*28)  # Flatten the input\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# ReLU-y를 사용하는 간단한 신경망 정의\n",
        "class NetReLU_Y(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NetReLU_Y, self).__init__()\n",
        "        self.fc1 = nn.Linear(28*28, 512)\n",
        "        self.fc2 = nn.Linear(512, 512)\n",
        "        self.fc3 = nn.Linear(512, 10)\n",
        "        self.relu_y = ReLU_Y()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28*28)  # Flatten the input\n",
        "        x = self.relu_y(self.fc1(x))\n",
        "        x = self.relu_y(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# 모델 초기화\n",
        "net_relu = NetReLU()\n",
        "net_relu_y = NetReLU_Y()\n",
        "\n",
        "# ReLU 모델의 초기 파라미터 부호 반전하여 ReLU-y 모델에 설정\n",
        "with torch.no_grad():\n",
        "    for param_relu, param_relu_y in zip(net_relu.parameters(), net_relu_y.parameters()):\n",
        "        param_relu_y.data = -torch.abs(param_relu.data) * torch.sign(param_relu.data)\n",
        "\n",
        "# 손실 함수와 옵티마이저 정의\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer_relu = optim.SGD(net_relu.parameters(), lr=0.01, momentum=0.9)\n",
        "optimizer_relu_y = optim.SGD(net_relu_y.parameters(), lr=0.01, momentum=0.9)\n",
        "\n",
        "# 모델 학습 함수\n",
        "def train_model(model, trainloader, criterion, optimizer, epochs=5):\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "        for inputs, labels in trainloader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "        print(f\"Epoch {epoch+1}, Loss: {running_loss/len(trainloader)}\")\n",
        "\n",
        "# 모델 평가 함수\n",
        "def test_model(model, testloader):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in testloader:\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    print(f'Accuracy: {100 * correct / total}%')\n",
        "\n",
        "# ReLU 네트워크 학습\n",
        "print(\"Training ReLU network...\")\n",
        "train_model(net_relu, trainloader, criterion, optimizer_relu, epochs=5)\n",
        "\n",
        "# ReLU-y 네트워크 학습\n",
        "print(\"Training ReLU-y network...\")\n",
        "train_model(net_relu_y, trainloader, criterion, optimizer_relu_y, epochs=5)\n",
        "\n",
        "# ReLU 네트워크 평가\n",
        "print(\"Testing ReLU network...\")\n",
        "test_model(net_relu, testloader)\n",
        "\n",
        "# ReLU-y 네트워크 평가\n",
        "print(\"Testing ReLU-y network...\")\n",
        "test_model(net_relu_y, testloader)\n",
        "\n",
        "# 모델 파라미터의 부호 비교 함수\n",
        "def compare_parameter_signs(model1, model2):\n",
        "    params1 = model1.named_parameters()\n",
        "    params2 = model2.named_parameters()\n",
        "\n",
        "    dict_params2 = dict(params2)\n",
        "    total_param_count = 0\n",
        "    different_sign_count = 0\n",
        "\n",
        "    for name1, param1 in params1:\n",
        "        if name1 in dict_params2:\n",
        "            param2 = dict_params2[name1]\n",
        "            sign1 = torch.sign(param1)\n",
        "            sign2 = torch.sign(param2)\n",
        "            total_param_count += param1.numel()  # 전체 파라미터 개수 추가\n",
        "            if not torch.equal(sign1, sign2):\n",
        "                different_sign_count += torch.sum(sign1 != sign2).item()  # 부호가 다른 파라미터 개수 추가\n",
        "                print(f\"Parameter {name1} has different signs.\")\n",
        "                print(f\"Model 1 - {name1}: {sign1}\")\n",
        "                print(f\"Model 2 - {name1}: {sign2}\")\n",
        "\n",
        "    print(f\"Total number of parameters: {total_param_count}\")\n",
        "    print(f\"Total number of parameters with different signs: {different_sign_count}\")\n",
        "\n",
        "print(\"\\nComparing parameter signs between ReLU and ReLU-y networks...\")\n",
        "compare_parameter_signs(net_relu, net_relu_y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R3P58QrE1YJp",
        "outputId": "4456513e-43a2-43eb-a89b-1291a1752c46"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training ReLU network...\n",
            "Epoch 1, Loss: 0.38039735969958277\n",
            "Epoch 2, Loss: 0.1529672586881339\n",
            "Epoch 3, Loss: 0.10826150237469435\n",
            "Epoch 4, Loss: 0.08443819391412108\n",
            "Epoch 5, Loss: 0.06907687925506256\n",
            "Training ReLU-y network...\n",
            "Epoch 1, Loss: 0.388705630435237\n",
            "Epoch 2, Loss: 0.15379621476026328\n",
            "Epoch 3, Loss: 0.10626411362231444\n",
            "Epoch 4, Loss: 0.08456459207468227\n",
            "Epoch 5, Loss: 0.06752357331699313\n",
            "Testing ReLU network...\n",
            "Accuracy: 97.57%\n",
            "Testing ReLU-y network...\n",
            "Accuracy: 97.11%\n",
            "\n",
            "Comparing parameter signs between ReLU and ReLU-y networks...\n",
            "Parameter fc1.weight has different signs.\n",
            "Model 1 - fc1.weight: tensor([[ 1.,  1., -1.,  ..., -1.,  1.,  1.],\n",
            "        [-1., -1.,  1.,  ..., -1., -1.,  1.],\n",
            "        [ 1.,  1.,  1.,  ..., -1., -1.,  1.],\n",
            "        ...,\n",
            "        [-1., -1.,  1.,  ...,  1.,  1., -1.],\n",
            "        [ 1.,  1.,  1.,  ..., -1., -1.,  1.],\n",
            "        [-1.,  1., -1.,  ...,  1., -1., -1.]], grad_fn=<SignBackward0>)\n",
            "Model 2 - fc1.weight: tensor([[-1., -1.,  1.,  ...,  1., -1., -1.],\n",
            "        [ 1.,  1., -1.,  ...,  1., -1., -1.],\n",
            "        [-1., -1., -1.,  ...,  1.,  1., -1.],\n",
            "        ...,\n",
            "        [ 1.,  1., -1.,  ..., -1., -1.,  1.],\n",
            "        [-1., -1., -1.,  ...,  1.,  1., -1.],\n",
            "        [ 1., -1.,  1.,  ..., -1.,  1.,  1.]], grad_fn=<SignBackward0>)\n",
            "Parameter fc1.bias has different signs.\n",
            "Model 1 - fc1.bias: tensor([-1., -1., -1.,  1.,  1.,  1.,  1.,  1.,  1., -1.,  1.,  1.,  1.,  1.,\n",
            "         1.,  1., -1.,  1., -1.,  1., -1., -1.,  1., -1., -1.,  1.,  1., -1.,\n",
            "         1.,  1., -1.,  1., -1., -1.,  1., -1.,  1.,  1.,  1., -1., -1., -1.,\n",
            "         1.,  1., -1.,  1., -1.,  1., -1.,  1.,  1.,  1., -1.,  1., -1., -1.,\n",
            "         1.,  1., -1.,  1., -1.,  1., -1.,  1.,  1., -1.,  1., -1.,  1., -1.,\n",
            "        -1.,  1., -1., -1.,  1.,  1.,  1., -1.,  1.,  1., -1., -1., -1., -1.,\n",
            "        -1.,  1.,  1., -1., -1.,  1.,  1.,  1., -1.,  1., -1., -1., -1.,  1.,\n",
            "         1., -1., -1.,  1., -1.,  1.,  1., -1.,  1., -1.,  1.,  1., -1.,  1.,\n",
            "        -1.,  1.,  1., -1., -1., -1., -1.,  1.,  1.,  1., -1.,  1.,  1., -1.,\n",
            "         1., -1.,  1.,  1.,  1.,  1.,  1., -1., -1.,  1., -1., -1., -1., -1.,\n",
            "        -1.,  1.,  1.,  1., -1., -1., -1.,  1.,  1., -1., -1., -1.,  1.,  1.,\n",
            "        -1., -1., -1.,  1.,  1., -1.,  1.,  1., -1.,  1.,  1., -1., -1., -1.,\n",
            "        -1., -1.,  1.,  1., -1.,  1.,  1., -1.,  1., -1.,  1., -1.,  1.,  1.,\n",
            "        -1., -1., -1., -1.,  1.,  1., -1.,  1., -1., -1., -1., -1., -1.,  1.,\n",
            "         1.,  1., -1.,  1.,  1.,  1., -1., -1., -1., -1.,  1., -1.,  1., -1.,\n",
            "        -1., -1., -1., -1., -1., -1., -1., -1., -1.,  1.,  1.,  1., -1.,  1.,\n",
            "        -1.,  1.,  1.,  1., -1., -1.,  1.,  1.,  1., -1., -1.,  1.,  1.,  1.,\n",
            "         1.,  1., -1.,  1.,  1., -1.,  1., -1.,  1., -1., -1.,  1., -1.,  1.,\n",
            "         1., -1.,  1., -1., -1., -1., -1., -1., -1., -1.,  1.,  1., -1., -1.,\n",
            "         1.,  1.,  1., -1.,  1., -1., -1., -1.,  1.,  1.,  1.,  1., -1.,  1.,\n",
            "         1.,  1.,  1., -1.,  1.,  1.,  1., -1.,  1.,  1.,  1.,  1.,  1., -1.,\n",
            "        -1.,  1.,  1., -1., -1.,  1.,  1., -1.,  1.,  1., -1.,  1.,  1., -1.,\n",
            "         1., -1., -1., -1.,  1.,  1., -1.,  1., -1., -1., -1., -1., -1., -1.,\n",
            "        -1.,  1., -1., -1.,  1.,  1.,  1.,  1., -1.,  1., -1.,  1., -1.,  1.,\n",
            "        -1.,  1., -1.,  1.,  1., -1., -1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
            "         1., -1.,  1., -1., -1., -1.,  1., -1., -1.,  1., -1., -1., -1., -1.,\n",
            "        -1.,  1., -1., -1., -1.,  1.,  1., -1.,  1.,  1., -1.,  1.,  1.,  1.,\n",
            "        -1., -1.,  1., -1., -1., -1.,  1., -1.,  1.,  1., -1.,  1.,  1., -1.,\n",
            "         1.,  1.,  1.,  1., -1., -1., -1., -1., -1.,  1., -1., -1.,  1., -1.,\n",
            "        -1.,  1., -1., -1.,  1.,  1.,  1., -1., -1., -1.,  1., -1.,  1.,  1.,\n",
            "        -1.,  1., -1., -1.,  1.,  1.,  1.,  1., -1.,  1., -1.,  1., -1., -1.,\n",
            "        -1.,  1.,  1.,  1.,  1., -1.,  1., -1., -1., -1., -1., -1.,  1., -1.,\n",
            "        -1.,  1., -1.,  1.,  1.,  1., -1.,  1., -1.,  1., -1., -1.,  1., -1.,\n",
            "        -1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1., -1.,  1., -1., -1.,  1.,\n",
            "        -1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1., -1.,  1., -1.,  1., -1.,\n",
            "         1.,  1., -1., -1.,  1.,  1.,  1., -1.,  1.,  1., -1., -1.,  1.,  1.,\n",
            "         1., -1., -1.,  1.,  1., -1.,  1., -1.], grad_fn=<SignBackward0>)\n",
            "Model 2 - fc1.bias: tensor([ 1.,  1.,  1., -1., -1., -1., -1., -1., -1.,  1., -1., -1., -1., -1.,\n",
            "        -1., -1.,  1., -1., -1., -1.,  1.,  1., -1.,  1.,  1., -1., -1.,  1.,\n",
            "         1., -1.,  1., -1.,  1.,  1., -1.,  1., -1., -1., -1.,  1.,  1.,  1.,\n",
            "        -1., -1.,  1., -1.,  1., -1.,  1., -1., -1., -1.,  1., -1.,  1.,  1.,\n",
            "        -1., -1.,  1., -1.,  1., -1.,  1., -1., -1.,  1., -1.,  1., -1.,  1.,\n",
            "         1., -1.,  1.,  1., -1., -1., -1.,  1., -1., -1.,  1.,  1.,  1.,  1.,\n",
            "         1., -1., -1.,  1.,  1., -1., -1., -1.,  1., -1.,  1.,  1.,  1., -1.,\n",
            "        -1.,  1.,  1., -1.,  1., -1., -1.,  1., -1.,  1., -1., -1.,  1., -1.,\n",
            "         1., -1., -1.,  1.,  1.,  1.,  1., -1., -1., -1.,  1., -1., -1.,  1.,\n",
            "        -1.,  1., -1., -1., -1., -1., -1.,  1.,  1., -1.,  1.,  1.,  1.,  1.,\n",
            "         1., -1., -1., -1.,  1.,  1.,  1.,  1., -1.,  1.,  1.,  1., -1., -1.,\n",
            "         1.,  1.,  1.,  1., -1.,  1., -1., -1.,  1.,  1., -1.,  1.,  1.,  1.,\n",
            "         1.,  1., -1., -1.,  1., -1., -1.,  1., -1.,  1., -1.,  1., -1., -1.,\n",
            "         1.,  1.,  1.,  1., -1., -1.,  1., -1.,  1.,  1., -1.,  1.,  1., -1.,\n",
            "        -1., -1.,  1., -1., -1., -1.,  1.,  1.,  1.,  1., -1.,  1., -1.,  1.,\n",
            "         1.,  1.,  1., -1.,  1.,  1.,  1.,  1.,  1., -1., -1., -1.,  1.,  1.,\n",
            "         1., -1., -1., -1.,  1.,  1., -1., -1.,  1.,  1.,  1., -1., -1., -1.,\n",
            "        -1., -1.,  1., -1., -1.,  1., -1.,  1., -1.,  1.,  1., -1.,  1., -1.,\n",
            "        -1.,  1., -1.,  1.,  1.,  1.,  1.,  1.,  1.,  1., -1., -1.,  1.,  1.,\n",
            "        -1., -1., -1.,  1., -1.,  1.,  1.,  1., -1., -1., -1., -1.,  1., -1.,\n",
            "        -1., -1., -1.,  1., -1., -1., -1.,  1., -1., -1., -1., -1., -1.,  1.,\n",
            "        -1., -1., -1.,  1.,  1., -1., -1.,  1., -1., -1.,  1., -1., -1.,  1.,\n",
            "        -1.,  1.,  1.,  1., -1., -1.,  1., -1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
            "         1., -1.,  1.,  1., -1., -1., -1., -1.,  1., -1.,  1., -1.,  1., -1.,\n",
            "         1., -1.,  1., -1., -1.,  1.,  1., -1., -1., -1., -1., -1., -1., -1.,\n",
            "        -1.,  1., -1.,  1.,  1.,  1., -1.,  1., -1., -1.,  1.,  1.,  1.,  1.,\n",
            "         1., -1.,  1.,  1.,  1., -1., -1.,  1., -1., -1.,  1., -1., -1.,  1.,\n",
            "         1.,  1., -1.,  1.,  1.,  1., -1.,  1., -1., -1.,  1., -1., -1., -1.,\n",
            "        -1., -1., -1., -1.,  1.,  1.,  1.,  1.,  1., -1.,  1.,  1., -1.,  1.,\n",
            "         1., -1.,  1.,  1., -1., -1., -1.,  1.,  1.,  1., -1.,  1., -1.,  1.,\n",
            "         1., -1.,  1.,  1., -1., -1., -1., -1.,  1., -1.,  1., -1.,  1.,  1.,\n",
            "         1., -1., -1., -1., -1.,  1., -1.,  1.,  1.,  1.,  1.,  1., -1.,  1.,\n",
            "         1., -1.,  1., -1., -1., -1.,  1., -1.,  1., -1.,  1.,  1., -1.,  1.,\n",
            "         1., -1., -1., -1., -1., -1., -1., -1., -1.,  1., -1.,  1.,  1., -1.,\n",
            "         1., -1., -1., -1., -1., -1., -1., -1., -1.,  1., -1.,  1., -1.,  1.,\n",
            "        -1., -1.,  1.,  1., -1., -1., -1.,  1., -1., -1.,  1.,  1., -1., -1.,\n",
            "        -1.,  1.,  1., -1., -1.,  1., -1.,  1.], grad_fn=<SignBackward0>)\n",
            "Parameter fc2.weight has different signs.\n",
            "Model 1 - fc2.weight: tensor([[ 1.,  1., -1.,  ...,  1., -1.,  1.],\n",
            "        [ 1.,  1., -1.,  ..., -1., -1.,  1.],\n",
            "        [-1.,  1., -1.,  ...,  1., -1., -1.],\n",
            "        ...,\n",
            "        [ 1., -1.,  1.,  ..., -1.,  1., -1.],\n",
            "        [-1., -1.,  1.,  ...,  1., -1., -1.],\n",
            "        [ 1.,  1., -1.,  ...,  1.,  1., -1.]], grad_fn=<SignBackward0>)\n",
            "Model 2 - fc2.weight: tensor([[-1., -1.,  1.,  ..., -1.,  1., -1.],\n",
            "        [-1., -1.,  1.,  ...,  1.,  1.,  1.],\n",
            "        [ 1., -1., -1.,  ..., -1.,  1.,  1.],\n",
            "        ...,\n",
            "        [-1.,  1., -1.,  ...,  1., -1.,  1.],\n",
            "        [ 1.,  1., -1.,  ..., -1.,  1., -1.],\n",
            "        [-1., -1.,  1.,  ...,  1., -1.,  1.]], grad_fn=<SignBackward0>)\n",
            "Parameter fc2.bias has different signs.\n",
            "Model 1 - fc2.bias: tensor([ 1., -1.,  1.,  1., -1., -1.,  1.,  1.,  1.,  1.,  1.,  1., -1., -1.,\n",
            "         1.,  1.,  1., -1.,  1.,  1., -1., -1.,  1., -1.,  1.,  1.,  1.,  1.,\n",
            "        -1., -1.,  1.,  1., -1., -1.,  1., -1., -1., -1.,  1.,  1.,  1., -1.,\n",
            "         1.,  1., -1.,  1.,  1., -1.,  1.,  1.,  1.,  1., -1.,  1., -1.,  1.,\n",
            "         1., -1., -1.,  1.,  1., -1.,  1., -1.,  1., -1., -1.,  1., -1., -1.,\n",
            "         1., -1.,  1.,  1., -1.,  1.,  1., -1., -1.,  1., -1.,  1.,  1., -1.,\n",
            "        -1.,  1.,  1., -1., -1., -1., -1.,  1., -1., -1.,  1.,  1.,  1., -1.,\n",
            "         1.,  1., -1., -1., -1.,  1.,  1.,  1.,  1.,  1., -1.,  1., -1., -1.,\n",
            "         1., -1., -1., -1.,  1., -1., -1.,  1.,  1., -1., -1.,  1., -1.,  1.,\n",
            "         1., -1.,  1., -1.,  1., -1., -1.,  1., -1., -1., -1.,  1.,  1.,  1.,\n",
            "         1., -1.,  1.,  1., -1., -1., -1.,  1.,  1.,  1., -1., -1.,  1., -1.,\n",
            "        -1.,  1., -1.,  1., -1., -1., -1.,  1., -1.,  1.,  1.,  1.,  1., -1.,\n",
            "        -1., -1.,  1., -1.,  1.,  1., -1.,  1.,  1., -1., -1.,  1., -1.,  1.,\n",
            "        -1.,  1., -1., -1.,  1.,  1.,  1., -1., -1.,  1., -1., -1., -1., -1.,\n",
            "        -1.,  1., -1., -1., -1.,  1., -1., -1.,  1.,  1., -1., -1.,  1., -1.,\n",
            "        -1.,  1., -1., -1., -1., -1.,  1., -1., -1.,  1.,  1.,  1., -1., -1.,\n",
            "         1., -1.,  1., -1., -1., -1., -1.,  1.,  1., -1.,  1., -1.,  1.,  1.,\n",
            "         1.,  1.,  1.,  1.,  1., -1., -1., -1., -1., -1.,  1., -1.,  1., -1.,\n",
            "         1.,  1.,  1.,  1.,  1.,  1.,  1., -1.,  1.,  1.,  1.,  1.,  1., -1.,\n",
            "         1.,  1., -1.,  1., -1., -1., -1.,  1., -1.,  1., -1.,  1., -1.,  1.,\n",
            "        -1.,  1.,  1.,  1., -1.,  1.,  1., -1., -1.,  1., -1.,  1., -1.,  1.,\n",
            "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
            "        -1.,  1., -1., -1., -1., -1.,  1.,  1.,  1.,  1., -1.,  1.,  1., -1.,\n",
            "        -1.,  1.,  1., -1., -1., -1.,  1.,  1., -1.,  1., -1., -1.,  1., -1.,\n",
            "        -1., -1.,  1., -1.,  1., -1.,  1.,  1.,  1.,  1., -1.,  1.,  1., -1.,\n",
            "        -1., -1., -1.,  1., -1.,  1.,  1.,  1., -1.,  1., -1., -1.,  1., -1.,\n",
            "        -1.,  1., -1., -1.,  1.,  1.,  1., -1., -1., -1.,  1.,  1.,  1., -1.,\n",
            "         1., -1.,  1., -1., -1., -1., -1., -1.,  1.,  1.,  1.,  1., -1.,  1.,\n",
            "        -1.,  1.,  1.,  1.,  1.,  1., -1., -1.,  1., -1., -1., -1., -1., -1.,\n",
            "        -1., -1., -1., -1.,  1., -1., -1.,  1., -1.,  1.,  1.,  1.,  1.,  1.,\n",
            "        -1., -1.,  1., -1., -1., -1., -1., -1.,  1., -1., -1., -1., -1., -1.,\n",
            "         1., -1.,  1.,  1.,  1.,  1.,  1.,  1.,  1., -1., -1., -1.,  1.,  1.,\n",
            "         1., -1.,  1., -1., -1.,  1.,  1., -1.,  1.,  1.,  1.,  1., -1., -1.,\n",
            "        -1., -1.,  1.,  1.,  1., -1., -1., -1.,  1., -1.,  1.,  1., -1.,  1.,\n",
            "        -1.,  1.,  1., -1.,  1.,  1.,  1., -1., -1.,  1., -1., -1.,  1.,  1.,\n",
            "        -1., -1., -1.,  1., -1.,  1., -1.,  1., -1.,  1.,  1.,  1.,  1.,  1.,\n",
            "         1.,  1.,  1., -1.,  1., -1.,  1., -1.], grad_fn=<SignBackward0>)\n",
            "Model 2 - fc2.bias: tensor([-1.,  1., -1., -1.,  1.,  1., -1., -1., -1., -1., -1., -1.,  1.,  1.,\n",
            "        -1., -1., -1.,  1., -1., -1.,  1.,  1., -1.,  1., -1., -1.,  1., -1.,\n",
            "         1.,  1.,  1., -1.,  1.,  1., -1.,  1.,  1.,  1., -1., -1.,  1.,  1.,\n",
            "        -1., -1.,  1., -1., -1.,  1., -1., -1., -1., -1.,  1., -1.,  1., -1.,\n",
            "         1.,  1.,  1., -1., -1.,  1.,  1.,  1., -1.,  1.,  1.,  1.,  1.,  1.,\n",
            "        -1.,  1., -1., -1.,  1.,  1., -1.,  1.,  1., -1.,  1., -1., -1.,  1.,\n",
            "         1., -1., -1.,  1.,  1.,  1., -1., -1., -1.,  1.,  1., -1., -1.,  1.,\n",
            "        -1., -1.,  1.,  1.,  1., -1., -1., -1.,  1.,  1., -1., -1.,  1.,  1.,\n",
            "        -1., -1.,  1., -1., -1., -1.,  1., -1., -1.,  1., -1., -1., -1., -1.,\n",
            "        -1.,  1., -1.,  1., -1., -1.,  1., -1.,  1.,  1.,  1., -1., -1., -1.,\n",
            "        -1.,  1., -1., -1.,  1.,  1.,  1., -1.,  1., -1., -1.,  1., -1.,  1.,\n",
            "         1., -1.,  1., -1.,  1.,  1.,  1., -1.,  1., -1., -1., -1., -1.,  1.,\n",
            "         1.,  1., -1.,  1., -1., -1.,  1., -1.,  1.,  1.,  1., -1.,  1., -1.,\n",
            "         1., -1.,  1.,  1., -1., -1., -1.,  1.,  1., -1.,  1.,  1.,  1.,  1.,\n",
            "         1., -1.,  1.,  1.,  1., -1., -1.,  1., -1., -1.,  1.,  1., -1., -1.,\n",
            "        -1., -1., -1.,  1.,  1.,  1., -1.,  1.,  1., -1., -1., -1.,  1.,  1.,\n",
            "        -1., -1., -1.,  1.,  1.,  1.,  1., -1., -1.,  1.,  1.,  1., -1., -1.,\n",
            "        -1., -1., -1.,  1., -1.,  1.,  1.,  1.,  1., -1., -1.,  1., -1.,  1.,\n",
            "        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,  1.,\n",
            "        -1., -1.,  1., -1.,  1.,  1.,  1., -1.,  1., -1.,  1., -1.,  1., -1.,\n",
            "         1.,  1., -1., -1.,  1., -1., -1.,  1., -1., -1., -1., -1.,  1., -1.,\n",
            "        -1., -1., -1., -1., -1., -1., -1., -1.,  1., -1., -1., -1., -1.,  1.,\n",
            "         1., -1.,  1.,  1.,  1.,  1., -1., -1., -1., -1.,  1., -1., -1.,  1.,\n",
            "         1., -1.,  1.,  1.,  1.,  1., -1., -1.,  1., -1.,  1.,  1., -1.,  1.,\n",
            "         1., -1.,  1.,  1., -1.,  1., -1., -1.,  1., -1.,  1., -1., -1.,  1.,\n",
            "         1.,  1.,  1., -1.,  1., -1., -1., -1.,  1., -1.,  1.,  1., -1.,  1.,\n",
            "         1.,  1.,  1.,  1., -1.,  1.,  1.,  1.,  1.,  1., -1., -1., -1.,  1.,\n",
            "        -1.,  1., -1.,  1.,  1.,  1.,  1.,  1., -1., -1., -1., -1.,  1., -1.,\n",
            "         1., -1., -1., -1., -1., -1.,  1.,  1., -1.,  1.,  1.,  1., -1.,  1.,\n",
            "         1.,  1.,  1.,  1., -1.,  1.,  1., -1.,  1., -1., -1., -1., -1., -1.,\n",
            "         1.,  1., -1.,  1.,  1., -1.,  1.,  1., -1.,  1.,  1.,  1.,  1.,  1.,\n",
            "        -1.,  1., -1., -1., -1.,  1., -1., -1., -1.,  1.,  1.,  1., -1., -1.,\n",
            "        -1., -1., -1.,  1.,  1., -1., -1.,  1., -1., -1., -1., -1.,  1.,  1.,\n",
            "         1.,  1., -1., -1., -1.,  1.,  1.,  1., -1.,  1., -1., -1.,  1., -1.,\n",
            "         1., -1.,  1.,  1., -1., -1., -1.,  1.,  1., -1.,  1.,  1., -1., -1.,\n",
            "         1.,  1.,  1., -1.,  1., -1.,  1.,  1.,  1., -1., -1., -1., -1., -1.,\n",
            "        -1., -1., -1.,  1., -1.,  1., -1.,  1.], grad_fn=<SignBackward0>)\n",
            "Parameter fc3.weight has different signs.\n",
            "Model 1 - fc3.weight: tensor([[-1., -1.,  1.,  ..., -1.,  1.,  1.],\n",
            "        [ 1., -1., -1.,  ..., -1.,  1., -1.],\n",
            "        [-1.,  1.,  1.,  ...,  1.,  1.,  1.],\n",
            "        ...,\n",
            "        [-1.,  1.,  1.,  ...,  1.,  1.,  1.],\n",
            "        [ 1., -1., -1.,  ..., -1., -1., -1.],\n",
            "        [-1.,  1.,  1.,  ..., -1., -1., -1.]], grad_fn=<SignBackward0>)\n",
            "Model 2 - fc3.weight: tensor([[ 1.,  1., -1.,  ..., -1.,  1.,  1.],\n",
            "        [ 1.,  1.,  1.,  ...,  1., -1.,  1.],\n",
            "        [ 1.,  1.,  1.,  ..., -1., -1., -1.],\n",
            "        ...,\n",
            "        [ 1.,  1.,  1.,  ..., -1., -1., -1.],\n",
            "        [-1.,  1.,  1.,  ...,  1., -1.,  1.],\n",
            "        [-1., -1., -1.,  ..., -1.,  1.,  1.]], grad_fn=<SignBackward0>)\n",
            "Parameter fc3.bias has different signs.\n",
            "Model 1 - fc3.bias: tensor([-1., -1., -1.,  1.,  1., -1., -1., -1.,  1.,  1.],\n",
            "       grad_fn=<SignBackward0>)\n",
            "Model 2 - fc3.bias: tensor([-1., -1., -1., -1.,  1.,  1., -1., -1.,  1.,  1.],\n",
            "       grad_fn=<SignBackward0>)\n",
            "Total number of parameters: 669706\n",
            "Total number of parameters with different signs: 586632\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uksag8DkIN74"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}