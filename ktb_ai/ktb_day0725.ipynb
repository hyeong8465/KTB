{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNITLAYG4ExOEr7npTlLU5V",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hyeong8465/KTB/blob/main/ktb_ai/ktb_day0725.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "86J-J4StNoDb"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "생성형 AI 1\n",
        "\"attention is all you need\" 시간을 들여서 이해하는 것 강추\n",
        "\n",
        "- 생성형 AI - Generative AI\n",
        "기존 데이터를 학습하여 새로운 데이터(이미지,텍스트, 음악, 비디오, ...)를 생성하는 인공지능 기술\n",
        "Diffusion, GAN, VAE, Transformer, ...\n",
        "\n",
        "생성형 AI를 통해 산업의 방향이 바뀌었다.\n",
        "\n",
        "대규모 언어 모델(Large Language Model, LLM)\n",
        "방대한 양의 텍스트 데이터를 학습하여 자연어를 이해하고 생성할 수 있는 인공지능 모델\n",
        "GPT-3, GPT-4\n",
        "응용분야: 자연어 생성, 질의응답, 언어 번역, 텍스트 요약, 그 외의 자연어와 관련된 작업\n",
        "\n",
        "Transformer 모델\n",
        "RNN이나 LSTM과 달리, Transformer는 순차적인 데이터 처리 없이 병렬로 처리 가능\n",
        "기본 개념: Self-attention, Encoder-Decoder 구조\n",
        "\n",
        "- Attention Is All You Need\n",
        "introduction\n",
        "RNN, LSTM, Attention Mechanism, Transformer 제안\n",
        "\n",
        "Model Architecture\n",
        "인코더-디코더 구조\n",
        "인코더 스택: 6개의 레이어 구성, 멀티-헤드 self-attention 메커니즘과 위치별 완전 연결 피드\n",
        "Scaled Dot-Product Attention\n",
        "Multi-Head Attention\n",
        "Position-wise Feed-Forward Networks\n",
        "Learned Embedding\n",
        "Positional Encoding\n",
        "\n",
        "Why Self-Attention\n",
        "self-attention vs 순환 및 합성곱 레이어 비교:\n",
        "    레이어당 계산 복잡성에 이점\n",
        "    최소한의 순차적 연산 수로 계산 과정 병렬화\n",
        "    장거리 의존성 경로 길이에 큰 이점\n",
        "Self Attention의 장점:\n",
        "    모든 위치를 일정한 수의 순차적 연산으로 연결, 순환 레이어는 O(n) 순차적 연산 필요\n",
        "    시퀀스 길이 n이 표현 차원 d보다 작을 때 Self-Attention 레이어의 계산 속도가 순환 레이어보다 빠름\n",
        "    매우 긴 시퀀스에서 Self-Attention은 크기 r의 이웃만을 고려하도록 제한하는 것이 가능 ->\n",
        "\n",
        "\n",
        "\n",
        "논문 공부 방법\n",
        "1. 베이스 논문부터 시작\n",
        "2. 최신 모델 논문부터 시작 -> 모르는 부분 레퍼런스 참고\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oohrudt20Mqi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}