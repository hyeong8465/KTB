{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNkvSmxhM8Xb/Mqt0JiMwiu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hyeong8465/KTB/blob/main/ktb_ai/ktb_day0726.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n5gu5U4qVDWL"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "프롬프트 엔지니어링\n",
        "\n",
        "- LLM(Large Language Model)\n",
        "자연어에 대한 확률 모델\n",
        "자연어를 확률적으로 나열\n",
        "활용: 기계 번역, 음성 인식, 검색, 문자 인식, 자연어 생성\n",
        "\n",
        "hyper marameter\n",
        "temperature(온도)\n",
        "    창의성 결정 변수, 출력 자연어의 확률을 보정하는 변수\n",
        "    낮을수록 딱딱하고 단조롭고 일관적인 답변\n",
        "\n",
        "Top_p\n",
        "    창의성 결정 변수, 누적 확률을 제한하는 변수\n",
        "    높을수록 다양한 답변을 얻을 수 있다.\n",
        "\n",
        "Maximun_length: 대화의 크기(형태소 단위)\n",
        "Frequency Penalty: 동일 단어의 등장 빈도에 따라 페널티(단조로움 회피)\n",
        "Presence Penalty: 동일 단어의 존재에 따라 페널티(다양한 화제)\n",
        "\n",
        "- Prompt Engineering\n",
        "프롬프트: 특정 작업을 수행하도록 LLM에 요청하는 자연어 텍스트\n",
        "프롬프트 엔지니어링: 생성형 AI 솔루션을 안내하여 원하는 결과를 생성하는 프로세스\n",
        "\n",
        "How can I be good at asking questions to AI?\n",
        "구체적이고 명확한 지시\n",
        "명령의 배경을 설명\n",
        "원하는 결과에 대한 예시 설명\n",
        "단계별 가이드 제시\n",
        "제약사항 설명\n",
        "결과의 작성 형식을 구체적으로 명시\n",
        "스스로 생각하며 일할 수 있도록 지시\n",
        "\n",
        "Basic Tips\n",
        "페르소나: LLM에 역할을 부여하여 LLM의 확률 계산에 영향을 줌\n",
        "구분기호: 구분 기호를 통해 해당 내용이 어떤 맥락인지 명확히 파악하도록 지시\n",
        "제약사항 및 형식 지정: 확률 계산에 미리 제약\n",
        "구체적이고 명확한 지시(중요): 최대한 구체적으로 지시해야 LLM의 확률 계산 결과에 오류를 줄일 수 있음\n",
        "\n",
        "LLM이 코드를 학습한 게 아니므로 코드 질문에 제대로 답변하지 못할 수도 있음.\n",
        "-> 페르소나: IT기업 파이썬 경력 5년 이상 시니어 개발자\n",
        "-> 구분기호: #### 코드 #### 질문\n",
        "-> 제약사항 및 형식지정\n",
        "-> 구체적이고 명확한 지시\n",
        "\n",
        "Few-Shot: 예제를 제공하는 것으로 모델의 출력에 프롬프트의 예시가 영향을 끼칠 수 있게 함\n",
        "Chain of Thought(CoT):\n",
        "    복잡한 질문을 더 작고 논리적인 부분으로 나누는 기법\n",
        "    문제를 해결하는 중간 단계를 거치면서 문제를 올바르게 해결한 자연어를 출력할 가능성이 높아짐\n",
        "    Few-shot을 주로 사용\n",
        "    Zero-shot의 경우 가장 유명한 프롬프트 \"Let's think step by step.\"\n",
        "Self-Consistency(SC):\n",
        "    Chain of Thoght를 확장시킨 개념\n",
        "    Few-shot CoT\n",
        "    여러 모델의 결과를 Voting\n",
        "Tree of Thoughts:\n",
        "    너비 우선 탐색 + 깊이 우선 탐색\n",
        "    백트래킹의 개념 사용\n",
        "    1. 생각의 분해(트리의 레벨, depth)\n",
        "    2. 생각 만들기(노드)\n",
        "    3. 생각 평가(가지치기)\n",
        "\n",
        "ReAct: Reasoning(추론) + Acting(행동)\n",
        "    인간의 행동 양식에서 영감을 받음\n",
        "    추론 -> 계획/생성 -> 행동 -> 관찰(지각) -> 추론의 반복\n",
        "    추론: 현재 상황에 대한 추론\n",
        "    행동: 검색, 조회, 종료\n",
        "    관찰: 행동의 결과를 객관적으로 관찰\n",
        "\n",
        "- Advanced Prompt Engineering\n",
        "Batch Prompting\n",
        "    프롬프트를 batch 형태로 입력\n",
        "    장점: 빠른 속도, 일관된 답변, 결과 성능 파악에 용이\n",
        "    단점: 복잡한 프롬프트의 경우 배치 프롬프트 제작이 어려움, 모든 답변에 오류가 퍼질 수 있음\n",
        "Prompting Chaining(여러 모델 사용)\n",
        "    복잡한 하나의 프롬프트를 단순한 여러 개의 프롬프트로 나누어 엮는 방법\n",
        "    입력 -> 모델 -> 출력 -> 모델 -> 출력\n",
        "    장점: 각 모델의 전문성, 유연성(모듈화), 효율성(특정 부분만 파인튜닝 가능)\n",
        "Prompt Injection\n",
        "    사용자가 프롬프트를 조작해 편향되거나 악의적인 결과를 생성하려는 공격\n",
        "    예방: 너무 짧은 프롬프트 지양, 추측하기 복잡하고 어려운 프롬프트 작성, 입력/출력에 필터링, 프롬프트를 일정 시기마다 업데이트하고 수정\n",
        "\n",
        "RAG\n",
        "    할루시네이션을 방지하는 대표적 방법\n",
        "    RAG는 일종의 오픈북 테스트 -> 팩을 찾아서 답안을 작성\n",
        "    일반 LLM은 모르는 내용은 못 적거나 틀린 답을 못적음\n",
        "    프롬프트 입력 -> 프롬프트 내용으로 검색 -> 프롬프트 내용과 관련된 내용 추천 -> 프롬프트 + 검색 결과를 LLM에 입력 -> 결과 출력\n",
        "\n",
        "- 마무리\n",
        "LLM은 자비스가 아니다\n",
        "    학습된 자연어 안에서 유사한 패턴을 확률에 기반해서 출력\n",
        "    논리적으로 이해하지 못함\n",
        "    추론, 수리 등의 분야는 성능을 기대하기 어렵다\n",
        "    파인튜닝은 지식을 넣는 개념보다는 단어의 확률, 패턴에 대한 정보를 학습하는 개념\n",
        "\n",
        "Prompt Engineering에 Silver Bullet은 없다.\n",
        "    모델의 아키텍처, 학습 데이터, 모델의 변화에 따라 프롬프트 엔지니어링은 변할 수 있다.\n",
        "    절대적인 방법은 없다\n",
        "    많은 시도를 통해 서비스에 적합한 결과를 내는 프롬프트와 타협\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 실습\n",
        "Chatgpt에게서 원하는 결과값 도출하기\n",
        "1. 어투 일관성  \n",
        "'-해요'체로만 반환\n",
        "2. 새로운 유의한 데이터 생성  \n",
        "특정 알고리즘 문제를 넣고 유사한 알고리즘 생성"
      ],
      "metadata": {
        "id": "Cwgo9wGWpkVZ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "G7cqY1JfpjGt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}